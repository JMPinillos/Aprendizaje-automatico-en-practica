{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ☁️ Lab activity 1: Unsupervised clustering"
      ],
      "metadata": {
        "id": "Iw5P-NiPACW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author**: [Andres J. Sanchez-Fernandez](https://www.unir.net/profesores/andres-jesus-sanchez-fernandez/)\n",
        "\n",
        "**References**:\n",
        "\n",
        "> KMEANS. (n.d.). Scikit-learn. https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "\n",
        "> Demo of DBSCAN clustering algorithm. (n.d.). *Scikit-learn*. https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html\n"
      ],
      "metadata": {
        "id": "YV4y9RWfUFwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives of this session\n",
        "\n",
        "By the end of this lab, students will be able to:\n",
        "\n",
        "1. Understand the concept of unsupervised learning and how it differs from supervised approaches.\n",
        "\n",
        "1. Apply clustering algorithms such as K-Means or DBSCAN to real datasets.\n",
        "\n",
        "1. Preprocess and explore datasets to identify appropriate features for clustering.\n",
        "\n",
        "1. Interpret clustering results using visualizations and relevant metrics (e.g., silhouette score).\n",
        "\n",
        "1. Justify the choice of a clustering algorithm based on the characteristics of the data.\n",
        "\n",
        "1. Evaluate the performance and limitations of different clustering methods."
      ],
      "metadata": {
        "id": "ktJrz4giE3wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the required libraries"
      ],
      "metadata": {
        "id": "Jy3Ne-QPAJP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When executing locally.\n",
        "!pip3 install numpy pandas matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "x2zbchJwAbja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required libraries.\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import random\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    calinski_harabasz_score,\n",
        "    davies_bouldin_score,\n",
        "    adjusted_rand_score\n",
        ")\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "# Reproducibility.\n",
        "seed = 42\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "vDW-IH1sBEQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make your exported script work even in standard Python environments, add a fallback for display():"
      ],
      "metadata": {
        "id": "Ewb48ET5FKWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fallback if IPython is not available.\n",
        "try:\n",
        "    from IPython.display import display\n",
        "except ImportError:\n",
        "    def display(obj):\n",
        "        print(obj)"
      ],
      "metadata": {
        "id": "lEwfClSYFL2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing steps"
      ],
      "metadata": {
        "id": "uc6hqKhDFWzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target _dataset_"
      ],
      "metadata": {
        "id": "I0t9qnNTBusT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Tsanas, A. & Xifara, A. (2012). Energy Efficiency [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C51307.\n",
        "\n"
      ],
      "metadata": {
        "id": "XO5eC2CODw4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. It can also be used as a multi-class classification problem if the response is rounded to the nearest integer.\n",
        "\n"
      ],
      "metadata": {
        "id": "r3LIxP-4c3hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL to download the dataset.\n",
        "dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx'"
      ],
      "metadata": {
        "id": "9XYssrrsDVHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset into a Pandas DataFrame"
      ],
      "metadata": {
        "id": "-DjWJq_VFgTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset into a pandas dataframe.\n",
        "df = pd.read_excel(dataset_url)\n",
        "\n",
        "# Display the whole dataframe.\n",
        "display(df)"
      ],
      "metadata": {
        "id": "M-m3SyO7FtMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data cleaning and inspection"
      ],
      "metadata": {
        "id": "Ss1mQMReS43N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define array of column names.\n",
        "feature_names = [\n",
        "    'Relative_Compactness',\n",
        "    'Surface_Area',\n",
        "    'Wall_Area',\n",
        "    'Roof_Area',\n",
        "    'Overall_Height',\n",
        "    'Orientation',\n",
        "    'Glazing_Area',\n",
        "    'Glazing_Area_Distribution',\n",
        "    'Heating_Load',\n",
        "    'Cooling_Load'\n",
        "]\n",
        "\n",
        "# Replace column names.\n",
        "df.columns = feature_names"
      ],
      "metadata": {
        "id": "QwdSpbuZS-9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data types and missing values.\n",
        "print('Dataset info:')\n",
        "display(df.info())\n",
        "\n",
        "# Parse all columns automatically based on their values (not necessary here).\n",
        "df = df.convert_dtypes()"
      ],
      "metadata": {
        "id": "WvDGxDFOg-l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Orientation'].value_counts()"
      ],
      "metadata": {
        "id": "pPHVR2eZfdVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect numerical data.\n",
        "print('Summary statistics:')\n",
        "display(df.describe())"
      ],
      "metadata": {
        "id": "sCLxNDgIk0ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values per column.\n",
        "print('Missing values per column:')\n",
        "display(df.isna().sum())"
      ],
      "metadata": {
        "id": "FzCv9d7uk3kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicated rows.\n",
        "print(f'Number of duplicated rows: {df.duplicated().sum()}')\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "nb_t_0NKi2E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select features"
      ],
      "metadata": {
        "id": "70qR6R6-llBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical features"
      ],
      "metadata": {
        "id": "nCuSVIVW8lpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop categorical features for clustering.\n",
        "df_clust = df.drop(columns=['Orientation', 'Glazing_Area_Distribution'])"
      ],
      "metadata": {
        "id": "cxyOCg8u8kZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analyze correlations"
      ],
      "metadata": {
        "id": "8IkF5aG58wRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful function to find pairs of variables with absolute correlation above the threshold.\n",
        "def find_highly_correlated_pairs(\n",
        "    corr_matrix: pd.DataFrame,\n",
        "    threshold: float = 0.85\n",
        ") -> List[Tuple[str, str, float]]:\n",
        "    \"\"\"\n",
        "    Identifies all pairs of variables in the correlation matrix that have\n",
        "    an absolute Pearson correlation greater than the specified threshold.\n",
        "\n",
        "    Args:\n",
        "        corr_matrix (pd.DataFrame): A square correlation matrix (e.g., from df.corr()).\n",
        "        threshold (float): The minimum absolute correlation to consider.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, str, float]]: Pairs of column names and their correlation value.\n",
        "    \"\"\"\n",
        "    high_corr_pairs = []\n",
        "\n",
        "    # Iterate over the columns.\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "\n",
        "        # Avoid duplicate pairs and self-correlation.\n",
        "        for j in range(i):\n",
        "\n",
        "            # Get correlation value.\n",
        "            corr_value = corr_matrix.iloc[i, j]\n",
        "\n",
        "            # Check if high correlation.\n",
        "            if abs(corr_value) >= threshold:\n",
        "                col_i = corr_matrix.columns[i]\n",
        "                col_j = corr_matrix.columns[j]\n",
        "                high_corr_pairs.append((col_i, col_j, corr_value))\n",
        "\n",
        "    return high_corr_pairs"
      ],
      "metadata": {
        "id": "7gWv_oAV2nQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find columns with at least one absolute correlation above threshold.\n",
        "threshold = 0.9"
      ],
      "metadata": {
        "id": "DtQ4q_-33mHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute correlation matrix.\n",
        "df_pearson_corr = df_clust.corr(numeric_only=True, method='pearson')\n",
        "upper_triangle_mask = np.triu(np.ones_like(df_pearson_corr, dtype=bool))\n",
        "\n",
        "# Plot masked correlation heatmap.\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(\n",
        "    df_pearson_corr,\n",
        "    mask=upper_triangle_mask,\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    cmap='coolwarm',\n",
        "    vmin=-1, vmax=1,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'shrink': 0.8},\n",
        ")\n",
        "plt.title('Correlation matrix (Pearson) – Bottom triangle')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compute high-correlation pairs from the Pearson correlation matrix.\n",
        "high_corr_pairs = find_highly_correlated_pairs(df_pearson_corr, threshold)\n",
        "if high_corr_pairs:\n",
        "    print('Highly correlated variable pairs (Pearson):')\n",
        "    for var1, var2, corr in high_corr_pairs:\n",
        "        print(f' - {var1} and {var2}: correlation = {corr:.2f}')\n",
        "else:\n",
        "    print(f'No variable pairs with correlation above {threshold} using Pearson.')"
      ],
      "metadata": {
        "id": "5ZcAVIyQ2FGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop correlated features (could be dimensionality reduction as well).\n",
        "# 'Relative_Compactness' is derived from surface area and volume.\n",
        "# 'Heating_Load' is redundant with 'Cooling_Load'.\n",
        "# 'Roof_Area' is redundant with 'Overall_Height'.\n",
        "df_clust = df_clust.drop(columns=['Heating_Load', 'Relative_Compactness', 'Roof_Area'])"
      ],
      "metadata": {
        "id": "6v7M3R514dBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mask to hide the lower triangle.\n",
        "df_pearson_corr = df_clust.corr(numeric_only=True, method='pearson')\n",
        "upper_triangle_mask = np.triu(np.ones_like(df_pearson_corr, dtype=bool))\n",
        "\n",
        "# Plot masked correlation heatmap.\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(\n",
        "    df_pearson_corr,\n",
        "    mask=upper_triangle_mask,\n",
        "    annot=True,\n",
        "    fmt='.2f',\n",
        "    cmap='coolwarm',\n",
        "    vmin=-1, vmax=1,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'shrink': 0.8},\n",
        ")\n",
        "plt.title('Correlation matrix (Pearson) – Bottom triangle')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "45voNrln4r4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaned dataset.\n",
        "display(df_clust)"
      ],
      "metadata": {
        "id": "0NrgY2T3i4qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardize features"
      ],
      "metadata": {
        "id": "AUWNyqlcTIVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the pipeline.\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler())  # Step 1 in the pipeline.\n",
        "])\n",
        "\n",
        "# Fit and transform data.\n",
        "data_scaled = pipeline.fit_transform(df_clust)\n",
        "print(data_scaled)"
      ],
      "metadata": {
        "id": "K7l4Nfg_JVmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means"
      ],
      "metadata": {
        "id": "HEktIiWBJrqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine the optimal $k$ for K-Means"
      ],
      "metadata": {
        "id": "eMCKOqGgn_pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Elbow method\n",
        "\n",
        "The Elbow Method evaluates the optimal number of clusters by measuring the inertia (within-cluster sum of squares). The goal is to find the \"elbow point,\" where the inertia decreases significantly but levels off, indicating diminishing returns for additional clusters.\n",
        "\n",
        "Inertia measures the **within-cluster sum of squares (WCSS)** and evaluates how well the clustering minimizes the distance between data points and their assigned cluster centroid. It is a measure of the compactness of clusters.\n",
        "\n",
        "For a given clustering solution, inertia is calculated as:\n",
        "\n",
        "$$\n",
        "\\text{Inertia} = \\sum_{k=1}^{K} \\sum_{i \\in C_k} \\| x_i - \\mu_k \\|^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ K $: Number of clusters.\n",
        "- $ C_k $: The set of points in cluster $k$.\n",
        "- $ x_i $: A data point.\n",
        "- $ \\mu_k $: The centroid of cluster $k$.\n",
        "- $ \\| x_i - \\mu_k \\|^2 $: Squared Euclidean distance between a point and its cluster centroid."
      ],
      "metadata": {
        "id": "PzLG99zPoPV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Range of clusters to evaluate.\n",
        "k_range = range(2, 10)"
      ],
      "metadata": {
        "id": "A0rczibrB5kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the Elbow and Silhouette analysis at $k=2$, not $k=1$, since $k=1$ gives no useful clustering insight: inertia is always highest and the silhouette score is undefined. Only $k\\geq2$ reveals meaningful cluster structure."
      ],
      "metadata": {
        "id": "5NMC2H8mDpxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute K-Means and inertia for several ks.\n",
        "inertias = []\n",
        "\n",
        "# Iterate over the clusters.\n",
        "for k in k_range:\n",
        "\n",
        "    # Compute k-means for a given k.\n",
        "    kmeans = KMeans(n_clusters=k, random_state=seed)\n",
        "    kmeans.fit(data_scaled)\n",
        "\n",
        "    # Save inertia.\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Visualization of the elbow method.\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, inertias, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia (within-cluster sum of squares)')\n",
        "plt.title('Elbow method for k-means')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-k2kyWiuor5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The inertia **drops rapidly from $k=2$ to $k=6$**, after which the decrease slows down.\n",
        "\n",
        "* This suggests an **elbow at $k=6$**, which is often interpreted as the optimal point because adding more clusters beyond that brings diminishing returns in variance explained"
      ],
      "metadata": {
        "id": "xXEEABxc_0kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install kneed"
      ],
      "metadata": {
        "id": "g-JPdDJN9kcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kneed import KneeLocator\n",
        "\n",
        "# Using KneeLocator to automatically find the elbow point.\n",
        "kneedle = KneeLocator(k_range, inertias, curve='convex', direction='decreasing')\n",
        "print(f'Optimal number of clusters: {kneedle.elbow}')"
      ],
      "metadata": {
        "id": "1LX5pBhb9jSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Silhouette score\n",
        "\n",
        "The Silhouette Score measures the quality of clustering by comparing how similar each point is to its own cluster (cohesion) versus other clusters (separation). Scores range from -1 to 1, where higher scores (close to 1) indicate better-defined clusters.\n",
        "\n",
        "\n",
        "For each data point $i$:\n",
        "1. **Cohesion $a(i)$:**\n",
        "   - Compute the average distance between $i$ and all other points in its cluster.\n",
        "2. **Separation $b(i)$:**\n",
        "   - Compute the average distance between $i$ and all points in the nearest neighboring cluster (the next closest cluster to $i$).\n",
        "\n",
        "The silhouette score for point $i$ is:\n",
        "\n",
        "$$\n",
        "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
        "$$"
      ],
      "metadata": {
        "id": "Qb_h1FntpUx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute K-Means and silhouette_scores for several ks.\n",
        "silhouette_scores = []\n",
        "\n",
        "# Iterate over the clusters.\n",
        "for k in k_range:\n",
        "\n",
        "    # Compute k-means for a given k.\n",
        "    kmeans = KMeans(n_clusters=k, random_state=seed)\n",
        "    kmeans.fit(data_scaled)\n",
        "\n",
        "    # Save score.\n",
        "    silhouette_scores.append(silhouette_score(data_scaled, kmeans.labels_))\n",
        "\n",
        "# Visualization of silhouette score.\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, silhouette_scores, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.title('Silhouette analysis for k-means')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BXzBrIl1peQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The silhouette score is **highest at $k=2$**, meaning those clusters are the most compact and well-separated.\n",
        "\n",
        "* However, **$k=2$ may oversimplify the structure**, especially if you expect more natural groupings."
      ],
      "metadata": {
        "id": "hizWVCc_AMcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selection of optimal $k$"
      ],
      "metadata": {
        "id": "c6EaZqQOp7Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the elbow point is in 6 clusters, we will select $k=2$ to compare these clusters to those found by DBSCAN using ARI, which resulted in $k=2$ having the highest Silhouette score. **This is a conservative but insightful initial scenario.**"
      ],
      "metadata": {
        "id": "3zGFmIAje2H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the optimal number of clusters.\n",
        "k_optimal = 2"
      ],
      "metadata": {
        "id": "3pOZjxC4BHx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting found clusters"
      ],
      "metadata": {
        "id": "ccvGPOIugiRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying k-means.\n",
        "kmeans = KMeans(n_clusters=k_optimal, random_state=seed)\n",
        "\n",
        "# Compute and predict clusters for each sample in DataFrame.\n",
        "df['Cluster_k-means'] = kmeans.fit_predict(data_scaled)"
      ],
      "metadata": {
        "id": "zHR7i94LqB49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of samples per cluster.\n",
        "cluster_counts = df['Cluster_k-means'].value_counts().sort_index()\n",
        "\n",
        "# Plot.\n",
        "cluster_counts.plot(kind='bar', figsize=(8, 5))\n",
        "plt.xlabel('Cluster label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('K-means cluster sizes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WPZolfnAECmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-scaled centroids.\n",
        "centroids_not_scaled = df.groupby('Cluster_k-means').mean().round(2)"
      ],
      "metadata": {
        "id": "i2K4S0SzenhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot data points and centroids.\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=df['Heating_Load'], y=df['Cooling_Load'], hue=df['Cluster_k-means'], palette='Set1')\n",
        "plt.scatter(centroids_not_scaled['Heating_Load'], centroids_not_scaled['Cooling_Load'], color='black', marker='*', s=200, label='Non-scaled centroids')\n",
        "plt.title('Clusters with k-means')\n",
        "plt.xlabel('Heating load')\n",
        "plt.ylabel('Cooling load')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hzfMmucTEVMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of centroids"
      ],
      "metadata": {
        "id": "mkgzB0zVFdsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resulting non-scaled centroids.\n",
        "print('Cluster centroids not scaled:')\n",
        "display(centroids_not_scaled)"
      ],
      "metadata": {
        "id": "3NfMAfg0POu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See how clusters are distributed across feature pairs.\n",
        "sns.pairplot(df, hue='Cluster_k-means', diag_kind='hist', corner=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z45iq8s1gSuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "yOn9nw36ulSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tune hyperparameters"
      ],
      "metadata": {
        "id": "L0-8N_A7ye7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Situation                  | Suggested Action     |\n",
        "|---------------------------|----------------------|\n",
        "| Too many small clusters   | Increase `eps` slightly |\n",
        "| Too many noise points (-1)| Decrease `min_samples` |"
      ],
      "metadata": {
        "id": "-jXjT3h0Bb1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### minPts & epsilon with elbow method"
      ],
      "metadata": {
        "id": "B58GSRSozHmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set minimum number of points to form a cluster.\n",
        "minPts = data_scaled.shape[1] + 1\n",
        "print(f'Minimum number of points to form a cluster: {minPts}')"
      ],
      "metadata": {
        "id": "pQ8NX05Sz7SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set `k = minPts`. Then use the **k-distance plot** to visually identify the ideal **ε** value (look for the “elbow”)."
      ],
      "metadata": {
        "id": "6zGfTP652-ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use NearestNeighbors to compute distances to k-th nearest neighbors.\n",
        "k = minPts\n",
        "neighbors = NearestNeighbors(n_neighbors=k)\n",
        "neighbors_fit = neighbors.fit(data_scaled)\n",
        "distances, indices = neighbors_fit.kneighbors(data_scaled)\n",
        "\n",
        "# Sort the distances to the k-th neighbor (use for elbow/knee method).\n",
        "k_distances = np.sort(distances[:, -1])\n",
        "\n",
        "# Plot the k-distance graph.\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(\n",
        "    y=k_distances,\n",
        "    mode='lines+markers',\n",
        "    line=dict(color='royalblue'),\n",
        "    marker=dict(size=5),\n",
        "    name=f'{k}-th Nearest Neighbor Distance'\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f'{k}-Distance plot (for ε selection)',\n",
        "    xaxis_title=f'Points sorted by distance to {k}th nearest neighbor',\n",
        "    yaxis_title='Distance',\n",
        "    template='plotly_white',\n",
        "    hovermode='x unified',\n",
        "    height=400,\n",
        "    width=800\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "HGG3KbyGyhX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visually inspect the eps range on the graph to set a good grid search criteria."
      ],
      "metadata": {
        "id": "p452NPSOfeyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using KneeLocator to automatically find the elbow point.\n",
        "kneedle = KneeLocator(range(len(k_distances)), k_distances, curve='convex', direction='increasing')\n",
        "print(f'Elbow of eps: {k_distances[kneedle.elbow]}')"
      ],
      "metadata": {
        "id": "9k_zYherc_-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpeqIEGsvfOH"
      },
      "source": [
        "#### Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5fr7vzMs4HV"
      },
      "source": [
        "Grid search optimizes Silhouette Score, finding clusters that are internally tight and externally separated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTdWJFmnqBrH"
      },
      "outputs": [],
      "source": [
        "# Define parameter grids.\n",
        "eps_values = np.linspace(0.7, 2.0, 20)\n",
        "min_samples_values = range(data_scaled.shape[1]+1, data_scaled.shape[1]*2)\n",
        "\n",
        "# Variables to store best results.\n",
        "best_score = -1\n",
        "best_eps = None\n",
        "best_min_samples = None\n",
        "best_n_clusters = None\n",
        "best_model = None\n",
        "\n",
        "# Grid Search.\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "\n",
        "        # Fit model.\n",
        "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = model.fit_predict(data_scaled)\n",
        "\n",
        "        # Ignore noise.\n",
        "        mask = labels != -1\n",
        "        n_clusters = len(set(labels[mask]))\n",
        "\n",
        "        # Silhouette score needs at least 2 clusters.\n",
        "        if n_clusters <= 1:\n",
        "            continue\n",
        "\n",
        "        # Compute Silhouette.\n",
        "        try:\n",
        "            score = silhouette_score(data_scaled[mask], labels[mask])\n",
        "        except Exception as e:\n",
        "            print(f'Error at eps={eps:.2f}, min_samples={min_samples}: {e}')\n",
        "            continue\n",
        "        print(f'eps={eps:.2f}, min_samples={min_samples}, n_clusters={n_clusters}, Silhouette={score:.4f}')\n",
        "\n",
        "        # Save best result so far.\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_eps = eps\n",
        "            best_min_samples = min_samples\n",
        "            best_n_clusters = n_clusters\n",
        "            best_model = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiYG_J-etwCK"
      },
      "outputs": [],
      "source": [
        "# Final best parameters\n",
        "print('Best combination found:')\n",
        "print(f'eps = {best_eps:.4f}')\n",
        "print(f'min_samples = {best_min_samples}')\n",
        "print(f'Number of clusters = {best_n_clusters}')\n",
        "print(f'Best Silhouette score = {best_score:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting found clusters"
      ],
      "metadata": {
        "id": "vhc4KjvMga2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMWzidNou-Gh"
      },
      "outputs": [],
      "source": [
        "# Save labels and compute Silhouette without noise.\n",
        "df['Cluster_DBSCAN'] = best_model.labels_\n",
        "mask = best_model.labels_ != -1\n",
        "silhouette = silhouette_score(data_scaled[mask], df['Cluster_DBSCAN'][mask])\n",
        "print(f'Silhouette score: {silhouette:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of samples per cluster.\n",
        "cluster_counts = df['Cluster_DBSCAN'].value_counts().sort_index()\n",
        "\n",
        "# Plot.\n",
        "cluster_counts.plot(kind='bar', figsize=(8, 5))\n",
        "plt.xlabel('Cluster label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('DBSCAN cluster sizes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "61gsuMVgDppv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot data points.\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=df['Heating_Load'], y=df['Cooling_Load'], hue=df['Cluster_DBSCAN'], palette='Set1')\n",
        "plt.title('Clusters with DBSCAN')\n",
        "plt.xlabel('Heating load')\n",
        "plt.ylabel('Cooling load')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S4yYg00-3tCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation of average groups"
      ],
      "metadata": {
        "id": "G3MwWbIm37Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame.\n",
        "mean_groups = pd.DataFrame(df, columns=df.drop(columns=['Cluster_k-means']).columns).groupby('Cluster_DBSCAN').mean().round(2).reset_index()\n",
        "print('Cluster means:')\n",
        "display(mean_groups)"
      ],
      "metadata": {
        "id": "5l4fXxVD4BOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See how clusters are distributed across feature pairs.\n",
        "sns.pairplot(df, hue='Cluster_DBSCAN', diag_kind='hist', corner=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AMxYiYJUf7hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "G7pciDZ2NFP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show DataFrame with labels.\n",
        "clustering_labels = ['Cluster_k-means', 'Cluster_DBSCAN']\n",
        "display(df[clustering_labels].head())"
      ],
      "metadata": {
        "id": "ilDlOu2bNK1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Silhouette score**\n",
        "\n",
        "- Measures how similar a sample is to its own cluster compared to others.\n",
        "- **Ranges from -1 to 1**: higher values mean **better and well-separated** clusters.\n",
        "\n",
        "**Calinski-Harabasz index (CH score)**\n",
        "\n",
        "- Measures the ratio of between-cluster dispersion to within-cluster dispersion.\n",
        "- **Higher values** indicate **better-defined** and **more separated** clusters.\n",
        "\n",
        "**Davies-Bouldin index (DB score)**\n",
        "\n",
        "- Measures the average similarity between each cluster and its most similar one.\n",
        "- **Lower values** indicate **better clustering** with more distinct groups."
      ],
      "metadata": {
        "id": "eBQgHS2dSRO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize results list.\n",
        "results = []\n",
        "\n",
        "# Iterate over clustering label columns.\n",
        "for label_col in clustering_labels:\n",
        "\n",
        "    # Get clusters.\n",
        "    labels = df[label_col].values\n",
        "\n",
        "    # Remove noise.\n",
        "    mask = labels != -1\n",
        "\n",
        "    # Compute metrics.\n",
        "    silhouette = silhouette_score(data_scaled[mask], labels[mask])\n",
        "    calinski = calinski_harabasz_score(data_scaled[mask], labels[mask])\n",
        "    davies = davies_bouldin_score(data_scaled[mask], labels[mask])\n",
        "\n",
        "    # Append results.\n",
        "    results.append({\n",
        "        'Clustering': label_col,\n",
        "        'Silhouette': silhouette,\n",
        "        'Calinski-Harabasz': calinski,\n",
        "        'Davies-Bouldin': davies\n",
        "    })\n",
        "\n",
        "# Create summary DataFrame.\n",
        "metrics_df = pd.DataFrame(results)\n",
        "display(metrics_df)"
      ],
      "metadata": {
        "id": "iNpoYiHJNID4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy to avoid modifying the original.\n",
        "ranking_df = metrics_df.copy()\n",
        "\n",
        "# For each metric, compute the ranking.\n",
        "# For Silhouette and Calinski, higher is better, so descending.\n",
        "# For Davies, lower is better, so ascending.\n",
        "ranking_df['Silhouette_Rank'] = ranking_df['Silhouette'].rank(ascending=False).astype(int)\n",
        "ranking_df['Calinski_Rank'] = ranking_df['Calinski-Harabasz'].rank(ascending=False).astype(int)\n",
        "ranking_df['Davies_Rank'] = ranking_df['Davies-Bouldin'].rank(ascending=True).astype(int)\n",
        "\n",
        "# Keep only name and rankings.\n",
        "final_ranking = ranking_df[['Clustering', 'Silhouette_Rank', 'Calinski_Rank', 'Davies_Rank']]\n",
        "display(final_ranking)"
      ],
      "metadata": {
        "id": "o3FfsO_yDG4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Adjusted Rand Index.\n",
        "print(f\"ARI index: {adjusted_rand_score(df['Cluster_k-means'], df['Cluster_DBSCAN'])}\")"
      ],
      "metadata": {
        "id": "Zbl_2Mv7YD_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9woea8NiYHsi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}