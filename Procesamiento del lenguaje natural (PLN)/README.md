# Procesamiento del Lenguaje Natural (PLN)

Esta secci贸n del repositorio recoge pr谩cticas centradas en el desarrollo de modelos de aprendizaje profundo aplicados al procesamiento del lenguaje natural. Los trabajos se enfocan en tareas clave como la clasificaci贸n de texto y el reconocimiento de entidades nombradas, empleando t茅cnicas modernas como *word embeddings*, *LSTM*, y modelos basados en *transformers*.

Cada notebook implementa un flujo completo que incluye la vectorizaci贸n del texto, el dise帽o de arquitecturas espec铆ficas, el entrenamiento supervisado y la evaluaci贸n del rendimiento con m茅tricas adaptadas a la naturaleza ling眉铆stica del problema. Estas actividades reflejan el dominio de las t茅cnicas m谩s actuales de PLN y su aplicabilidad a contextos reales en espa帽ol.

---

## Notebooks

### Ь Reconocimiento de entidades nombradas (NER)

Este trabajo aborda la tarea de *Named Entity Recognition* sobre un corpus anotado en espa帽ol. A trav茅s de t茅cnicas de tokenizaci贸n y codificaci贸n *BIO*, se implementa un modelo secuencial con *LSTM* bidireccional y *embedding* de palabras para identificar entidades como nombres, lugares u organizaciones.  
El estudio incluye el an谩lisis del comportamiento del modelo con diferentes funciones de activaci贸n, tama帽os de ventana y par谩metros de regularizaci贸n, as铆 como la evaluaci贸n detallada con m茅tricas por clase y matriz de confusi贸n. Se demuestra la viabilidad del enfoque en contextos de procesamiento de informaci贸n estructurada y extracci贸n autom谩tica de conocimiento.

[ `Named_Entity_Recognition.ipynb`](1-Named_Entity_Recognition/Named_Entity_Recognition.ipynb)

---

###  Clasificaci贸n de texto con *word embeddings* y *transformers*

Este proyecto analiza la clasificaci贸n de mensajes en espa帽ol utilizando diferentes representaciones vectoriales del lenguaje. Se comparan dos enfoques principales: redes neuronales recurrentes con *word embeddings* entrenados desde cero, y el uso de *transformers* multiling眉es preentrenados mediante la arquitectura *BERT*.  
La actividad incluye el an谩lisis de m茅tricas de evaluaci贸n, visualizaci贸n de *attention maps*, impacto del *fine-tuning* y adaptaci贸n del tokenizador. Se realiza un an谩lisis comparativo entre las dos metodolog铆as, evidenciando la superioridad de los modelos *transformer* en tareas de clasificaci贸n sem谩ntica compleja.

[ `Word_embeddings_y_transformers_para_clasificaci贸n_de_texto.ipynb`](2-PLN_Clasificaci贸n_transformers/Word_embeddings_y_transformers_para_clasificaci贸n_de_texto.ipynb)

---


# Procesamiento del Lenguaje Natural (PLN)

Esta secci贸n del repositorio recopila pr谩cticas centradas en el desarrollo de modelos de aprendizaje autom谩tico aplicados al procesamiento del lenguaje natural, con especial 茅nfasis en el idioma espa帽ol. Las actividades abordan tareas clave como la clasificaci贸n de texto y el reconocimiento de entidades nombradas, combinando t茅cnicas tradicionales de vectorizaci贸n con modelos preentrenados basados en *transformers*.

Cada notebook sigue una estructura completa: desde la exploraci贸n inicial y el tratamiento del texto, hasta la construcci贸n y evaluaci贸n de modelos. Se analizan m茅tricas de rendimiento adaptadas al problema, y se incluyen reflexiones sobre el uso pr谩ctico de cada t茅cnica en entornos reales.

---

## Notebooks

### Ь Reconocimiento de entidades nombradas (NER)

Este trabajo aborda la tarea de *Named Entity Recognition* sobre un corpus anotado manualmente en espa帽ol. Tras aplicar t茅cnicas de preprocesamiento textual, se implementa una red neuronal densa sobre secuencias codificadas con etiquetas *BIO*, entrenada con una funci贸n de p茅rdida categ贸rica y m茅tricas de exactitud por clase.

El estudio se centra en comprender el comportamiento del modelo al identificar correctamente entidades como nombres, ubicaciones y organizaciones, y se acompa帽a de visualizaciones de predicciones y matrices de confusi贸n para cada clase.

[ `Named_Entity_Recognition.ipynb`](1-Named_Entity_Recognition/Named_Entity_Recognition.ipynb)

---

###  Clasificaci贸n de texto con *word embeddings* y *transformers*

Este proyecto compara dos aproximaciones para la clasificaci贸n de mensajes en espa帽ol. La primera utiliza una arquitectura basada en *word embeddings* y capas densas, mientras que la segunda incorpora un modelo *transformer* preentrenado (*BETO*), adaptado mediante *fine-tuning*.

El trabajo incluye la preparaci贸n del corpus, tokenizaci贸n con *AutoTokenizer*, vectorizaci贸n, entrenamiento supervisado y an谩lisis comparativo entre ambos enfoques. Se visualizan m茅tricas clave como la *exactitud*, la matriz de confusi贸n y las predicciones para evaluar el impacto de los embeddings frente al uso de modelos del estado del arte.

[ `Word_embeddings_y_transformers_para_clasificaci贸n_de_texto.ipynb`](2-PLN_Clasificaci贸n_transformers/Word_embeddings_y_transformers_para_clasificaci贸n_de_texto.ipynb)

---

<center>by <strong>Jose Manuel Pinillos</strong></center>





<center>by <strong>Jose Manuel Pinillos</strong></center>