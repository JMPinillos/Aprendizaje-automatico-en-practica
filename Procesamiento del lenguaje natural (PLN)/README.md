# Procesamiento del Lenguaje Natural (PLN)

Esta secci贸n del repositorio recoge pr谩cticas centradas en el desarrollo de modelos de aprendizaje profundo aplicados al procesamiento del lenguaje natural. Los trabajos se enfocan en tareas clave como la clasificaci贸n de texto y el reconocimiento de entidades nombradas, empleando t茅cnicas modernas como *word embeddings*, *LSTM*, y modelos basados en *transformers*.

Cada notebook implementa un flujo completo que incluye la vectorizaci贸n del texto, el dise帽o de arquitecturas espec铆ficas, el entrenamiento supervisado y la evaluaci贸n del rendimiento con m茅tricas adaptadas a la naturaleza ling眉铆stica del problema. Estas actividades reflejan el dominio de las t茅cnicas m谩s actuales de PLN y su aplicabilidad a contextos reales en espa帽ol.

---

## Notebooks

### Ь Reconocimiento de entidades nombradas (NER)

Este trabajo aborda la tarea de *Named Entity Recognition* sobre un corpus anotado en espa帽ol. A trav茅s de t茅cnicas de tokenizaci贸n y codificaci贸n *BIO*, se implementa un modelo secuencial con *LSTM* bidireccional y *embedding* de palabras para identificar entidades como nombres, lugares u organizaciones.  
El estudio incluye el an谩lisis del comportamiento del modelo con diferentes funciones de activaci贸n, tama帽os de ventana y par谩metros de regularizaci贸n, as铆 como la evaluaci贸n detallada con m茅tricas por clase y matriz de confusi贸n. Se demuestra la viabilidad del enfoque en contextos de procesamiento de informaci贸n estructurada y extracci贸n autom谩tica de conocimiento.

[ `Named_Entity_Recognition.ipynb`](1-Named_Entity_Recognition/Named_Entity_Recognition.ipynb)

---

###  Clasificaci贸n de texto con *word embeddings* y *transformers*

Este proyecto analiza la clasificaci贸n de mensajes en espa帽ol utilizando diferentes representaciones vectoriales del lenguaje. Se comparan dos enfoques principales: redes neuronales recurrentes con *word embeddings* entrenados desde cero, y el uso de *transformers* multiling眉es preentrenados mediante la arquitectura *BERT*.  
La actividad incluye el an谩lisis de m茅tricas de evaluaci贸n, visualizaci贸n de *attention maps*, impacto del *fine-tuning* y adaptaci贸n del tokenizador. Se realiza un an谩lisis comparativo entre las dos metodolog铆as, evidenciando la superioridad de los modelos *transformer* en tareas de clasificaci贸n sem谩ntica compleja.

[ `Word_embeddings_y_transformers_para_clasificaci贸n_de_texto.ipynb`](2-PLN_Clasificaci贸n_transformers/Word_embeddings_y_transformers_para_clasificaci贸n_de_texto.ipynb)

---

<center>by <strong>Jose Manuel Pinillos</strong></center>